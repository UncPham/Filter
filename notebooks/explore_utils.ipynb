{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_only\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pylogger, rich_utils\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from importlib.util import find_spec\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "from src.utils import pylogger, rich_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pylogger.get_pylogger(__name__)\n",
    "\n",
    "\n",
    "def task_wrapper(task_func: Callable) -> Callable:\n",
    "    \"\"\"Optional decorator that wraps the task function in extra utilities.\n",
    "\n",
    "    Makes multirun more resistant to failure.\n",
    "\n",
    "    Utilities:\n",
    "    - Calling the `utils.extras()` before the task is started\n",
    "    - Calling the `utils.close_loggers()` after the task is finished or failed\n",
    "    - Logging the exception if occurs\n",
    "    - Logging the output dir\n",
    "    \"\"\"\n",
    "\n",
    "    def wrap(cfg: DictConfig):\n",
    "\n",
    "        # execute the task\n",
    "        try:\n",
    "\n",
    "            # apply extra utilities\n",
    "            extras(cfg)\n",
    "\n",
    "            metric_dict, object_dict = task_func(cfg=cfg)\n",
    "\n",
    "        # things to do if exception occurs\n",
    "        except Exception as ex:\n",
    "\n",
    "            # save exception to `.log` file\n",
    "            log.exception(\"\")\n",
    "\n",
    "            # when using hydra plugins like Optuna, you might want to disable raising exception\n",
    "            # to avoid multirun failure\n",
    "            raise ex\n",
    "\n",
    "        # things to always do after either success or exception\n",
    "        finally:\n",
    "\n",
    "            # display output dir path in terminal\n",
    "            log.info(f\"Output dir: {cfg.paths.output_dir}\")\n",
    "\n",
    "            # close loggers (even if exception occurs so multirun won't fail)\n",
    "            close_loggers()\n",
    "\n",
    "        return metric_dict, object_dict\n",
    "\n",
    "    return wrap\n",
    "\n",
    "\n",
    "def extras(cfg: DictConfig) -> None:\n",
    "    \"\"\"Applies optional utilities before the task is started.\n",
    "\n",
    "    Utilities:\n",
    "    - Ignoring python warnings\n",
    "    - Setting tags from command line\n",
    "    - Rich config printing\n",
    "    \"\"\"\n",
    "\n",
    "    # return if no `extras` config\n",
    "    if not cfg.get(\"extras\"):\n",
    "        log.warning(\"Extras config not found! <cfg.extras=null>\")\n",
    "        return\n",
    "\n",
    "    # disable python warnings\n",
    "    if cfg.extras.get(\"ignore_warnings\"):\n",
    "        log.info(\"Disabling python warnings! <cfg.extras.ignore_warnings=True>\")\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # prompt user to input tags from command line if none are provided in the config\n",
    "    if cfg.extras.get(\"enforce_tags\"):\n",
    "        log.info(\"Enforcing tags! <cfg.extras.enforce_tags=True>\")\n",
    "        rich_utils.enforce_tags(cfg, save_to_file=True)\n",
    "\n",
    "    # pretty print config tree using Rich library\n",
    "    if cfg.extras.get(\"print_config\"):\n",
    "        log.info(\"Printing config tree with Rich! <cfg.extras.print_config=True>\")\n",
    "        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)\n",
    "\n",
    "\n",
    "def instantiate_callbacks(callbacks_cfg: DictConfig) -> List[Callback]:\n",
    "    \"\"\"Instantiates callbacks from config.\"\"\"\n",
    "    callbacks: List[Callback] = []\n",
    "\n",
    "    if not callbacks_cfg:\n",
    "        log.warning(\"No callback configs found! Skipping..\")\n",
    "        return callbacks\n",
    "\n",
    "    if not isinstance(callbacks_cfg, DictConfig):\n",
    "        raise TypeError(\"Callbacks config must be a DictConfig!\")\n",
    "\n",
    "    for _, cb_conf in callbacks_cfg.items():\n",
    "        if isinstance(cb_conf, DictConfig) and \"_target_\" in cb_conf:\n",
    "            log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "            callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def instantiate_loggers(logger_cfg: DictConfig) -> List[Logger]:\n",
    "    \"\"\"Instantiates loggers from config.\"\"\"\n",
    "    logger: List[Logger] = []\n",
    "\n",
    "    if not logger_cfg:\n",
    "        log.warning(\"No logger configs found! Skipping...\")\n",
    "        return logger\n",
    "\n",
    "    if not isinstance(logger_cfg, DictConfig):\n",
    "        raise TypeError(\"Logger config must be a DictConfig!\")\n",
    "\n",
    "    for _, lg_conf in logger_cfg.items():\n",
    "        if isinstance(lg_conf, DictConfig) and \"_target_\" in lg_conf:\n",
    "            log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "            logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "@rank_zero_only\n",
    "def log_hyperparameters(object_dict: dict) -> None:\n",
    "    \"\"\"Controls which config parts are saved by lightning loggers.\n",
    "\n",
    "    Additionally saves:\n",
    "    - Number of model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    hparams = {}\n",
    "\n",
    "    cfg = object_dict[\"cfg\"]\n",
    "    model = object_dict[\"model\"]\n",
    "    trainer = object_dict[\"trainer\"]\n",
    "\n",
    "    if not trainer.logger:\n",
    "        log.warning(\"Logger not found! Skipping hyperparameter logging...\")\n",
    "        return\n",
    "\n",
    "    hparams[\"model\"] = cfg[\"model\"]\n",
    "\n",
    "    # save number of model parameters\n",
    "    hparams[\"model/params/total\"] = sum(p.numel() for p in model.parameters())\n",
    "    hparams[\"model/params/trainable\"] = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad\n",
    "    )\n",
    "    hparams[\"model/params/non_trainable\"] = sum(\n",
    "        p.numel() for p in model.parameters() if not p.requires_grad\n",
    "    )\n",
    "\n",
    "    hparams[\"data\"] = cfg[\"data\"]\n",
    "    hparams[\"trainer\"] = cfg[\"trainer\"]\n",
    "\n",
    "    hparams[\"callbacks\"] = cfg.get(\"callbacks\")\n",
    "    hparams[\"extras\"] = cfg.get(\"extras\")\n",
    "\n",
    "    hparams[\"task_name\"] = cfg.get(\"task_name\")\n",
    "    hparams[\"tags\"] = cfg.get(\"tags\")\n",
    "    hparams[\"ckpt_path\"] = cfg.get(\"ckpt_path\")\n",
    "    hparams[\"seed\"] = cfg.get(\"seed\")\n",
    "\n",
    "    # send hparams to all loggers\n",
    "    for logger in trainer.loggers:\n",
    "        logger.log_hyperparams(hparams)\n",
    "\n",
    "\n",
    "def get_metric_value(metric_dict: dict, metric_name: str) -> float:\n",
    "    \"\"\"Safely retrieves value of the metric logged in LightningModule.\"\"\"\n",
    "\n",
    "    if not metric_name:\n",
    "        log.info(\"Metric name is None! Skipping metric value retrieval...\")\n",
    "        return None\n",
    "\n",
    "    if metric_name not in metric_dict:\n",
    "        raise Exception(\n",
    "            f\"Metric value not found! <metric_name={metric_name}>\\n\"\n",
    "            \"Make sure metric name logged in LightningModule is correct!\\n\"\n",
    "            \"Make sure `optimized_metric` name in `hparams_search` config is correct!\"\n",
    "        )\n",
    "\n",
    "    metric_value = metric_dict[metric_name].item()\n",
    "    log.info(f\"Retrieved metric value! <{metric_name}={metric_value}>\")\n",
    "\n",
    "    return metric_value\n",
    "\n",
    "\n",
    "def close_loggers() -> None:\n",
    "    \"\"\"Makes sure all loggers closed properly (prevents logging failure during multirun).\"\"\"\n",
    "\n",
    "    log.info(\"Closing loggers...\")\n",
    "\n",
    "    if find_spec(\"wandb\"):  # if wandb is installed\n",
    "        import wandb\n",
    "\n",
    "        if wandb.run:\n",
    "            log.info(\"Closing wandb!\")\n",
    "            wandb.finish()\n",
    "\n",
    "\n",
    "@rank_zero_only\n",
    "def save_file(path: str, content: str) -> None:\n",
    "    \"\"\"Save file in rank zero mode (only on one process in multi-GPU setup).\"\"\"\n",
    "    with open(path, \"w+\") as file:\n",
    "        file.write(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
